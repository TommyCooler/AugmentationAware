# Augmentation-Aware Anomaly Detection System

Há»‡ thá»‘ng phÃ¡t hiá»‡n dá»‹ thÆ°á»ng (Anomaly Detection) hai giai Ä‘oáº¡n sá»­ dá»¥ng Contrastive Learning vÃ  Reconstruction-based Detection.

---

## ğŸ“‹ Má»¥c Lá»¥c

1. [Tá»•ng Quan Kiáº¿n TrÃºc](#tá»•ng-quan-kiáº¿n-trÃºc)
2. [Phase 1: Contrastive Learning Training](#phase-1-contrastive-learning-training)
3. [Phase 2: Supervised Reconstruction Training](#phase-2-supervised-reconstruction-training)
4. [Inference: Anomaly Detection](#inference-anomaly-detection)
5. [Cáº¥u TrÃºc Dá»± Ãn](#cáº¥u-trÃºc-dá»±-Ã¡n)
6. [HÆ°á»›ng Dáº«n Sá»­ Dá»¥ng](#hÆ°á»›ng-dáº«n-sá»­-dá»¥ng)

---

## ğŸ—ï¸ Tá»•ng Quan Kiáº¿n TrÃºc

Há»‡ thá»‘ng Ä‘Æ°á»£c chia thÃ nh 2 phase training:

### Phase 1: Contrastive Learning
- **Má»¥c Ä‘Ã­ch**: Huáº¥n luyá»‡n module **Augmentation** vÃ  **Encoder** Ä‘á»ƒ há»c biá»ƒu diá»…n (representations) tá»‘t tá»« dá»¯ liá»‡u normal
- **PhÆ°Æ¡ng phÃ¡p**: Self-supervised contrastive learning vá»›i NTXent loss
- **Äáº§u vÃ o**: Chá»‰ dÃ¹ng **train data** (khÃ´ng cÃ³ labels)
- **Äáº§u ra**: Checkpoint chá»©a Augmentation module Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n

### Phase 2: Supervised Reconstruction
- **Má»¥c Ä‘Ã­ch**: Huáº¥n luyá»‡n module **AGF-TCN** Ä‘á»ƒ reconstruct augmented data
- **PhÆ°Æ¡ng phÃ¡p**: Supervised reconstruction vá»›i MSE loss
- **Äáº§u vÃ o**: 
  - Augmentation module tá»« Phase 1 (Ä‘Ã£ Ä‘Æ°á»£c **freeze** - khÃ´ng train)
  - Train data (Ä‘á»ƒ huáº¥n luyá»‡n AGF-TCN)
- **Äáº§u ra**: Checkpoint chá»©a cáº£ Augmentation vÃ  AGF-TCN

### Inference
- **Má»¥c Ä‘Ã­ch**: PhÃ¡t hiá»‡n dá»‹ thÆ°á»ng trÃªn test data
- **CÆ¡ cháº¿**: TÃ­nh reconstruction error â†’ Anomaly score â†’ Threshold â†’ Prediction
- **Äáº§u vÃ o**: Test data + labels (Ä‘á»ƒ Ä‘Ã¡nh giÃ¡)
- **Äáº§u ra**: Anomaly scores, predictions, metrics, vÃ  visualization

---

## ğŸ”· Phase 1: Contrastive Learning Training

### Má»¥c TiÃªu
Huáº¥n luyá»‡n **Augmentation module** vÃ  **Encoder** Ä‘á»ƒ há»c Ä‘Æ°á»£c biá»ƒu diá»…n tá»‘t tá»« dá»¯ liá»‡u normal mÃ  khÃ´ng cáº§n labels.

### Modules ÄÆ°á»£c Train

#### 1. **Augmentation Module** (`modules/augmentation.py`)
- **Kiáº¿n trÃºc**: 
  - Transformer-based augmentation vá»›i learnable transformations
  - Gumbel-Softmax sampling Ä‘á»ƒ táº¡o cÃ¡c augmentation strategies khÃ¡c nhau
- **Input**: `(batch, n_channels, window_size)`
- **Output**: `(batch, n_channels, window_size)` - augmented version cá»§a input
- **Tham sá»‘ trainable**: âœ… CÃ³ (Ä‘Æ°á»£c train trong Phase 1)

#### 2. **Encoder** (`phase1/encoder.py`)
- **Kiáº¿n trÃºc**: CÃ³ 2 loáº¡i
  - **MLPEncoder**: Multi-layer perceptron vá»›i projection head
  - **CNNEncoder**: Convolutional encoder vá»›i projection head
- **Input**: `(batch, n_channels, window_size)`
- **Output**: `(batch, projection_dim)` - embedding vector
- **Tham sá»‘ trainable**: âœ… CÃ³ (Ä‘Æ°á»£c train trong Phase 1)

### Quy TrÃ¬nh Training

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 1 Training Flow                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Data Loading
   â”œâ”€ Load train data tá»« nhiá»u datasets (ucr, smd, etc.)
   â”œâ”€ Chia thÃ nh sliding windows (window_size, stride)
   â””â”€ Táº¡o dataloader vá»›i batch_size

2. Forward Pass (má»—i batch)
   â”œâ”€ Input: batch_windows (batch, channels, window_size)
   â”œâ”€ Augmentation: augmented = augmentation(batch_windows)
   â”œâ”€ Encoding: 
   â”‚   â”œâ”€ z_original = encoder(batch_windows)
   â”‚   â””â”€ z_augmented = encoder(augmented)
   â””â”€ Loss: NTXentLoss(z_original, z_augmented)

3. Backward Pass
   â”œâ”€ TÃ­nh gradient cho cáº£ encoder vÃ  augmentation
   â”œâ”€ Gradient clipping (optional)
   â””â”€ Update weights

4. Checkpoint Saving
   â”œâ”€ LÆ°u encoder_state_dict
   â”œâ”€ LÆ°u augmentation_state_dict
   â”œâ”€ LÆ°u config (n_channels, window_size, etc.)
   â””â”€ LÆ°u optimizer vÃ  scheduler state
```

### Chi Tiáº¿t Implementation

#### Loss Function: NTXent Loss
```python
# Táº¡o positive pairs
embeddings = [z_original, z_augmented]  # (2*batch, proj_dim)
labels = [0, 1, 2, ..., 0, 1, 2, ...]  # Má»—i cáº·p (i, i+batch) lÃ  positive pair

# NTXent loss: Pull positive pairs together, push negatives apart
loss = NTXentLoss(temperature=0.07)
```

#### Training Script: `phase1/train_phase1.py`
```bash
python phase1/train_phase1.py
```

**Cáº¥u hÃ¬nh chÃ­nh**:
- `window_size`: KÃ­ch thÆ°á»›c window (vÃ­ dá»¥: 16)
- `stride`: BÆ°á»›c nháº£y giá»¯a cÃ¡c windows (vÃ­ dá»¥: 1)
- `batch_size`: KÃ­ch thÆ°á»›c batch (vÃ­ dá»¥: 64)
- `num_epochs`: Sá»‘ epoch (vÃ­ dá»¥: 1000)
- `encoder_type`: 'mlp' hoáº·c 'cnn'
- `projection_dim`: Chiá»u cá»§a embedding (vÃ­ dá»¥: 256)
- `transformer_d_model`: Hidden dimension cá»§a transformer (vÃ­ dá»¥: 128)
- `transformer_nhead`: Sá»‘ attention heads (vÃ­ dá»¥: 2)

**Output**: 
- Checkpoint táº¡i `phase1/checkpoints/best_model.pth`
- Chá»©a augmentation module Ä‘Ã£ Ä‘Æ°á»£c train (dÃ¹ng cho Phase 2)

---

## ğŸ”· Phase 2: Supervised Reconstruction Training

### Má»¥c TiÃªu
Huáº¥n luyá»‡n **AGF-TCN** Ä‘á»ƒ reconstruct augmented data. Module nÃ y sáº½ Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ tÃ­nh reconstruction error trong inference.

### Modules ÄÆ°á»£c Train

#### 1. **Augmentation Module** (tá»« Phase 1)
- **Tráº¡ng thÃ¡i**: âŒ **FROZEN** (khÃ´ng train)
  - Táº¥t cáº£ parameters: `requires_grad = False`
  - Äáº·t vá» `eval()` mode (disable dropout, BN uses running stats)
- **Vai trÃ²**: Chá»‰ forward pass Ä‘á»ƒ táº¡o augmented data

#### 2. **AGF-TCN** (`phase2/agf_tcn.py`)
- **Kiáº¿n trÃºc**: 
  - Temporal Convolutional Network (TCN) vá»›i Adaptive Graph Fusion
  - Multi-scale feature extraction vÃ  fusion
  - Reconstruction head
- **Input**: `(batch, n_channels, window_size)` - augmented data
- **Output**: `(batch, n_channels, window_size)` - reconstructed data
- **Tham sá»‘ trainable**: âœ… CÃ³ (chá»‰ train module nÃ y)

### Quy TrÃ¬nh Training

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 2 Training Flow                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Load Phase 1 Checkpoint
   â”œâ”€ Load augmentation module (FROZEN)
   â””â”€ Verify augmentation is in eval mode

2. Data Loading
   â”œâ”€ Load train data (chá»‰ dÃ¹ng train, khÃ´ng dÃ¹ng test)
   â”œâ”€ Chia thÃ nh sliding windows
   â””â”€ Táº¡o train dataloader

3. Forward Pass (má»—i batch)
   â”œâ”€ Input: batch_data (batch, channels, window_size)
   â”œâ”€ Augmentation (FROZEN):
   â”‚   â””â”€ augmented = augmentation(batch_data)  # no_grad()
   â”œâ”€ Reconstruction:
   â”‚   â””â”€ reconstructed = agf_tcn(augmented)
   â””â”€ Loss: MSE(reconstructed, augmented)

4. Backward Pass
   â”œâ”€ TÃ­nh gradient CHá»ˆ cho agf_tcn (augmentation khÃ´ng cÃ³ gradient)
   â”œâ”€ Gradient clipping (optional)
   â””â”€ Update AGF-TCN weights

5. Checkpoint Saving
   â”œâ”€ LÆ°u augmentation_state_dict (frozen, tá»« Phase 1)
   â”œâ”€ LÆ°u agf_tcn_state_dict (trained)
   â”œâ”€ LÆ°u config (n_channels, window_size, agf_tcn config, etc.)
   â””â”€ LÆ°u metrics (train_loss, test_loss náº¿u cÃ³)
```

### Chi Tiáº¿t Implementation

#### Loss Function: MSE Loss
```python
# Augmented data tá»« frozen augmentation
augmented = augmentation(batch_data)  # (batch, channels, window_size)

# Reconstruction
reconstructed = agf_tcn(augmented)    # (batch, channels, window_size)

# MSE loss
loss = MSE(reconstructed, augmented)
```

#### Freezing Augmentation
```python
# Freeze táº¥t cáº£ parameters
for param in augmentation.parameters():
    param.requires_grad = False

# Set to eval mode (disable dropout, BN uses running stats)
augmentation.eval()

# Verify: trong forward pass, dÃ¹ng torch.no_grad()
with torch.no_grad():
    augmented = augmentation(batch_data)
```

#### Training Script: `phase2/train_phase2.py`
```bash
python phase2/train_phase2.py
```

**Cáº¥u hÃ¬nh chÃ­nh**:
- `phase1_checkpoint`: ÄÆ°á»ng dáº«n Ä‘áº¿n checkpoint Phase 1
- `window_size`: Pháº£i khá»›p vá»›i Phase 1
- `agf_tcn_channels`: Hidden channels cá»§a TCN (vÃ­ dá»¥: [64, 64])
- `dropout`: Dropout rate (vÃ­ dá»¥: 0.1)
- `activation`: Activation function (vÃ­ dá»¥: 'gelu')
- `fuse_type`: Loáº¡i fusion (vÃ­ dá»¥: 5 - TripConFusion)
- `batch_size`: KÃ­ch thÆ°á»›c batch (vÃ­ dá»¥: 64)
- `num_epochs`: Sá»‘ epoch (vÃ­ dá»¥: 50)

**Output**: 
- Checkpoint táº¡i `phase2/checkpoints/phase2_<dataset>_<subset>_best.pt`
- Chá»©a cáº£ augmentation (frozen) vÃ  agf_tcn (trained)
- DÃ¹ng cho inference

---

## ğŸ”· Inference: Anomaly Detection

### Má»¥c TiÃªu
PhÃ¡t hiá»‡n dá»‹ thÆ°á»ng trÃªn test data sá»­ dá»¥ng reconstruction error tá»« AGF-TCN.

### Quy TrÃ¬nh Inference

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Inference Flow                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Load Phase 2 Checkpoint
   â”œâ”€ Load augmentation module (FROZEN, eval mode)
   â”œâ”€ Load agf_tcn (FROZEN, eval mode)
   â””â”€ Load config (window_size, n_channels, etc.)

2. Data Loading
   â”œâ”€ Load test data vÃ  labels (per time-step)
   â”œâ”€ Chia thÃ nh sliding windows
   â””â”€ Táº¡o test dataloader (khÃ´ng shuffle)

3. Forward Pass (má»—i batch)
   â”œâ”€ Input: batch_data (batch, channels, window_size)
   â”œâ”€ Augmentation (FROZEN):
   â”‚   â””â”€ augmented = augmentation(batch_data)  # no_grad()
   â”œâ”€ Reconstruction:
   â”‚   â””â”€ reconstructed = agf_tcn(augmented)  # no_grad()
   â””â”€ Anomaly Score:
       â””â”€ score = MSE(reconstructed, augmented) per time-step
           # Shape: (batch, window_size) - 1 score per time-step

4. Map Window Scores to Time Series
   â”œâ”€ Window 0: Map táº¥t cáº£ time-step scores â†’ [0:window_size]
   â”œâ”€ Window i>0: Chá»‰ map score cá»§a time-step cuá»‘i â†’ [start+window_size-1]
   â””â”€ Forward fill NaN values
   
5. Threshold & Prediction
   â””â”€ Always search for best threshold (maximize F1) â†’ predictions = (anomaly_scores >= best_threshold).astype(int)

6. Evaluation (vá»›i Point Adjustment)
   â”œâ”€ Apply Point Adjustment: Náº¿u detect báº¥t ká»³ Ä‘iá»ƒm nÃ o trong segment
   â”‚   â†’ Mark toÃ n bá»™ segment lÃ  detected
   â”œâ”€ Compute metrics: F1, Precision, Recall, Accuracy
   â””â”€ Compute segment-level metrics

7. Visualization (optional)
   â”œâ”€ Plot original, augmented, reconstructed data
   â”œâ”€ Plot anomaly scores vá»›i threshold
   â”œâ”€ Highlight ground truth vÃ  predicted anomalies
   â””â”€ Save to results/visualizations/
```

### Chi Tiáº¿t Implementation

#### Anomaly Score Calculation
```python
# Trong má»—i window, tÃ­nh score cho tá»«ng time-step
timestep_losses = torch.mean((reconstructed - augmented) ** 2, dim=1)
# Shape: (batch, window_size) - 1 score per time-step

# Sau khi map vá» time series:
anomaly_scores  # Shape: (n_time_steps,)
```

#### Mapping Strategy
```python
def map_window_scores_to_timeseries(...):
    # Window 0: Map táº¥t cáº£
    timeseries_scores[0:window_size] = window_0_scores
    
    # Window i>0: Chá»‰ map time-step cuá»‘i
    last_idx = i * stride + window_size - 1
    timeseries_scores[last_idx] = window_i_scores[-1]
```

#### Threshold Search
```python
# LuÃ´n tÃ¬m threshold tá»‘t nháº¥t (maximize F1) vá»›i Point Adjustment
# evaluate_with_pa() tá»± Ä‘á»™ng search vÃ  tráº£ vá» best_threshold
metrics = evaluate_with_pa(
    anomaly_scores=anomaly_scores,
    labels=labels
)
# metrics['best_threshold'] chá»©a threshold tá»‘t nháº¥t
# metrics['predictions'] chá»©a predictions táº¡i best threshold
# Point Adjustment luÃ´n Ä‘Æ°á»£c Ã¡p dá»¥ng tá»± Ä‘á»™ng
```

#### Point Adjustment
```python
# Náº¿u cÃ³ báº¥t ká»³ prediction = 1 trong má»™t anomaly segment
# â†’ Set toÃ n bá»™ segment = 1
# GiÃºp Ä‘Ã¡nh giÃ¡ fair hÆ¡n (chá»‰ cáº§n detect 1 Ä‘iá»ƒm trong segment)
```

#### Inference Script: `phase2/inference.py`
```bash
# CÆ¡ báº£n
python phase2/inference.py --checkpoint phase2/checkpoints/phase2_ucr_135_best.pt

# TÃ¹y chá»‰nh
python phase2/inference.py \
    --checkpoint <path> \
    --dataset ucr \
    --subset 135 \
    --no_viz                   # Disable visualization
```

**Arguments**:
- `--checkpoint`: ÄÆ°á»ng dáº«n Ä‘áº¿n Phase 2 checkpoint (required)
- `--dataset`: TÃªn dataset (optional, láº¥y tá»« checkpoint náº¿u khÃ´ng cÃ³)
- `--subset`: Subset cá»§a dataset (optional, láº¥y tá»« checkpoint náº¿u khÃ´ng cÃ³)
- `--no_viz`: Táº¯t visualization (default: cÃ³ visualization)
- `--batch_size`: Batch size (optional, láº¥y tá»« checkpoint náº¿u khÃ´ng cÃ³)
- `--device`: cuda/cpu (optional, auto-detect náº¿u khÃ´ng cÃ³)

**LÆ°u Ã½**: 
- Há»‡ thá»‘ng luÃ´n tá»± Ä‘á»™ng search threshold tá»‘t nháº¥t (maximize F1). KhÃ´ng cÃ³ option Ä‘á»ƒ táº¯t hoáº·c set threshold cá»‘ Ä‘á»‹nh.
- Point Adjustment luÃ´n Ä‘Æ°á»£c Ã¡p dá»¥ng tá»± Ä‘á»™ng trong quÃ¡ trÃ¬nh Ä‘Ã¡nh giÃ¡.

**Outputs**:
1. **Metrics**: F1, Precision, Recall, Accuracy, Confusion Matrix
2. **Saved results**: `results/inference_<dataset>_<subset>_results.npz`
   - `anomaly_scores`: Anomaly scores cho má»—i time-step
   - `labels`: Ground truth labels
   - `metrics`: Táº¥t cáº£ metrics (dict)
3. **Visualization**: `results/visualizations/viz_<dataset>_<subset>.png`
   - Original data, augmented data, reconstructed data
   - Anomaly scores vá»›i threshold line
   - Highlighted anomaly regions (ground truth vÃ  predictions)

---

## ğŸ“ Cáº¥u TrÃºc Dá»± Ãn

```
ACIIDS2025/
â”œâ”€â”€ phase1/
â”‚   â”œâ”€â”€ train_phase1.py          # Phase 1 training script
â”‚   â”œâ”€â”€ encoder.py                # MLPEncoder, CNNEncoder
â”‚   â””â”€â”€ checkpoints/              # Phase 1 checkpoints
â”‚       â””â”€â”€ best_model.pth
â”‚
â”œâ”€â”€ phase2/
â”‚   â”œâ”€â”€ train_phase2.py           # Phase 2 training script
â”‚   â”œâ”€â”€ inference.py              # Inference script
â”‚   â”œâ”€â”€ visualize.py              # Visualization utilities
â”‚   â”œâ”€â”€ agf_tcn.py                # AGF-TCN model
â”‚   â”œâ”€â”€ basicBlock.py             # TCN basic blocks
â”‚   â”œâ”€â”€ FusionBlock.py            # Fusion blocks
â”‚   â””â”€â”€ checkpoints/              # Phase 2 checkpoints
â”‚       â””â”€â”€ phase2_*.pt
â”‚
â”œâ”€â”€ modules/
â”‚   â””â”€â”€ augmentation.py           # Augmentation module (Transformer-based)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dataloader.py             # Data loading functions
â”‚   â”œâ”€â”€ phase1_dataloader.py      # Phase 1 data preparation
â”‚   â”œâ”€â”€ phase2_dataloader.py      # Phase 2 data preparation
â”‚   â””â”€â”€ sliding_window.py         # Sliding window utilities
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ point_adjustment.py       # Point Adjustment evaluation
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ inference_*_results.npz   # Inference results
â”‚   â””â”€â”€ visualizations/           # Visualization images
â”‚       â””â”€â”€ viz_*.png
â”‚
â””â”€â”€ README.md                     # File nÃ y
```

---

## ğŸš€ HÆ°á»›ng Dáº«n Sá»­ Dá»¥ng

### 1. Phase 1 Training

```bash
cd <project_root>

# Chá»‰nh sá»­a config trong phase1/train_phase1.py
python phase1/train_phase1.py
```

**Config cáº§n chá»‰nh**:
- `datasets_info`: Danh sÃ¡ch datasets Ä‘á»ƒ train
- `window_size`, `stride`
- `batch_size`, `num_epochs`
- `encoder_type`: 'mlp' hoáº·c 'cnn'
- `transformer_d_model`, `transformer_nhead`

**Output**: `phase1/checkpoints/best_model.pth`

### 2. Phase 2 Training

```bash
# Chá»‰nh sá»­a config trong phase2/train_phase2.py
python phase2/train_phase2.py
```

**Config cáº§n chá»‰nh**:
- `phase1_checkpoint`: ÄÆ°á»ng dáº«n Ä‘áº¿n Phase 1 checkpoint
- `dataset_name`, `subset`: Dataset Ä‘á»ƒ train
- `window_size`: Pháº£i khá»›p vá»›i Phase 1
- `agf_tcn_channels`, `dropout`, `activation`, `fuse_type`

**Output**: `phase2/checkpoints/phase2_<dataset>_<subset>_best.pt`

### 3. Inference

```bash
# CÆ¡ báº£n (láº¥y dataset tá»« checkpoint)
python phase2/inference.py --checkpoint phase2/checkpoints/phase2_ucr_135_best.pt

# Vá»›i visualization (default)
python phase2/inference.py --checkpoint <path>

# KhÃ´ng visualization
python phase2/inference.py --checkpoint <path> --no_viz
```

### 4. Xem Káº¿t Quáº£

- **Metrics**: In ra console
- **Saved results**: `results/inference_<dataset>_<subset>_results.npz`
- **Visualization**: `results/visualizations/viz_<dataset>_<subset>.png`

---

## ğŸ“Š Metrics vÃ  Evaluation

### Metrics ÄÆ°á»£c TÃ­nh

1. **F1-Score**: Harmonic mean cá»§a Precision vÃ  Recall
2. **Precision**: TP / (TP + FP)
3. **Recall**: TP / (TP + FN)
4. **Accuracy**: (TP + TN) / (TP + TN + FP + FN)

### Point Adjustment (PA)

- **Má»¥c Ä‘Ã­ch**: ÄÃ¡nh giÃ¡ cÃ´ng báº±ng hÆ¡n cho time-series anomaly detection
- **CÆ¡ cháº¿**: Náº¿u detect báº¥t ká»³ Ä‘iá»ƒm nÃ o trong má»™t anomaly segment â†’ Mark toÃ n bá»™ segment lÃ  detected
- **LÃ½ do**: Trong thá»±c táº¿, chá»‰ cáº§n phÃ¡t hiá»‡n 1 Ä‘iá»ƒm trong segment lÃ  Ä‘á»§ Ä‘á»ƒ biáº¿t segment Ä‘Ã³ lÃ  anomaly

### Threshold Search

- **Má»¥c Ä‘Ã­ch**: TÃ¬m threshold tá»‘t nháº¥t Ä‘á»ƒ maximize F1-score
- **CÆ¡ cháº¿**: 
  - Duyá»‡t qua nhiá»u threshold values
  - TÃ­nh F1 vá»›i Point Adjustment
  - Chá»n threshold cÃ³ F1 cao nháº¥t

---

## ğŸ”§ LÆ°u Ã Ká»¹ Thuáº­t

### 1. Dropout vÃ  BatchNorm trong Inference
- Táº¥t cáº£ modules Ä‘á»u Ä‘Æ°á»£c set vá» `eval()` mode
- Dropout tá»± Ä‘á»™ng táº¯t
- BatchNorm dÃ¹ng running statistics (khÃ´ng update)

### 2. Gradient trong Inference
- DÃ¹ng `torch.no_grad()` Ä‘á»ƒ táº¯t gradient computation
- Tiáº¿t kiá»‡m memory vÃ  tÄƒng tá»‘c

### 3. Augmentation Freezing trong Phase 2
- Táº¥t cáº£ parameters: `requires_grad = False`
- LuÃ´n á»Ÿ `eval()` mode
- DÃ¹ng `torch.no_grad()` khi forward

### 4. Window to Time-Series Mapping
- Window 0: Map táº¥t cáº£ time-steps
- Window i>0: Chá»‰ map time-step cuá»‘i (khÃ´ng cÃ³ overlap vÃ¬ chá»‰ láº¥y time-step cuá»‘i)
- NaN: Forward fill

### 5. Data Normalization
- Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c normalize trong dataloader
- `normalized=True` khi load data

---

## ğŸ“ TÃ³m Táº¯t

### Phase 1: Contrastive Learning
- **Train**: Augmentation + Encoder
- **Loss**: NTXent Loss
- **Input**: Train data (khÃ´ng cáº§n labels)
- **Output**: Augmentation module Ä‘Ã£ train

### Phase 2: Supervised Reconstruction
- **Train**: AGF-TCN (Augmentation frozen)
- **Loss**: MSE Loss (reconstruction)
- **Input**: Train data + Phase 1 checkpoint
- **Output**: Augmentation (frozen) + AGF-TCN (trained)

### Inference
- **Forward**: Augmentation (frozen) â†’ AGF-TCN (frozen) â†’ Score
- **Evaluation**: Threshold search + Point Adjustment
- **Output**: Scores, predictions, metrics, visualization

---

## ğŸ“§ LiÃªn Há»‡

Náº¿u cÃ³ cÃ¢u há»i hoáº·c váº¥n Ä‘á», vui lÃ²ng táº¡o issue hoáº·c liÃªn há»‡ maintainer.

---

**Last Updated**: 2025

